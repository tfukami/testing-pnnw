{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Implementation of Pointer networks: http://arxiv.org/pdf/1506.03134v1.pdf.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import DataGenerator\n",
    "from pointer import pointer_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('batch_size', 32, 'Batch size.  ')\n",
    "flags.DEFINE_integer('max_steps', 10, 'Number of numbers to sort.  ')\n",
    "flags.DEFINE_integer('rnn_size', 32, 'RNN size.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointerNetwork(object):\n",
    "    \n",
    "    def __init__(self, max_len, input_size, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor):\n",
    "        \"\"\"Create the network. A simplified network that handles only sorting.\n",
    "        \n",
    "        Args:\n",
    "            max_len: maximum length of the model.\n",
    "            input_size: size of the inputs data.\n",
    "            size: number of units in each layer of the model.\n",
    "            num_layers: number of layers in the model.\n",
    "            max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "            batch_size: the size of the batches used during training;\n",
    "                the model construction is independent of batch_size, so it can be\n",
    "                changed after initialization if this is convenient, e.g., for decoding.\n",
    "            learning_rate: learning rate to start with.\n",
    "            learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "            \n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.decoder_targets = []\n",
    "        self.target_weights = []\n",
    "        for i in range(max_len):\n",
    "            self.encoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"EncoderInput%d\" % i))\n",
    "\n",
    "        for i in range(max_len + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"DecoderInput%d\" % i))\n",
    "            self.decoder_targets.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, max_len + 1], name=\"DecoderTarget%d\" % i))  # one hot\n",
    "            self.target_weights.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, 1], name=\"TargetWeight%d\" % i))\n",
    "\n",
    "            \n",
    "        # Encoder\n",
    "        \n",
    "        # Need for attention\n",
    "        encoder_outputs, final_state = tf.nn.rnn(cell, self.encoder_inputs, dtype = tf.float32)\n",
    "        \n",
    "        # Need a dummy output to point on it. End of decoding.\n",
    "        encoder_outputs = [tf.zeros([FLAGS.batch_size, FLAGS.rnn_size])] + encoder_outputs\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        top_states = [tf.reshape(e, [-1, 1, cell.output_size])\n",
    "                      for e in encoder_outputs]\n",
    "        attention_states = tf.concat(1, top_states)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            outputs, states, _ = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, cell)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=True):\n",
    "            predictions, _, inps = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, cell, feed_prev=True)\n",
    "            \n",
    "        self.predictions = predictions\n",
    "\n",
    "        self.outputs = outputs\n",
    "        self.inps = inps\n",
    "        # move code below to a separate function as in TF examples\n",
    "        \n",
    "            \n",
    "    def create_feed_dict(self, encoder_input_data, decoder_input_data, decoder_target_data):\n",
    "        feed_dict = {}\n",
    "        for placeholder, data in zip(self.encoder_inputs, encoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_inputs, decoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_targets, decoder_target_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder in self.target_weights:\n",
    "            feed_dict[placeholder] = np.ones([self.batch_size, 1])\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        loss = 0.0\n",
    "        for output, target, weight in zip(self.outputs, self.decoder_targets, self.target_weights):\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(output, target) * weight\n",
    "\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        test_loss = 0.0\n",
    "        for output, target, weight in zip(self.predictions, self.decoder_targets, self.target_weights):\n",
    "            test_loss += tf.nn.softmax_cross_entropy_with_logits(output, target) * weight\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        train_loss_value = 0.0\n",
    "        test_loss_value = 0.0\n",
    "        \n",
    "        correct_order = 0\n",
    "        all_order = 0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            merged = tf.merge_all_summaries()\n",
    "            writer = tf.train.SummaryWriter(\"/tmp/pointer_logs\", sess.graph)\n",
    "            init = tf.initialize_all_variables()\n",
    "            sess.run(init)\n",
    "            for i in range(10000):\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps)\n",
    "\n",
    "                # Train\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                d_x, l = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                train_loss_value = 0.9 * train_loss_value + 0.1 * d_x\n",
    "                #l = sess.run(train_op, feed_dict=feed_dict)\n",
    "                #train_loss_valiue = 0.9 * train_loss_value\n",
    "                                \n",
    "                if i % 100 == 0:\n",
    "                    print('Step: %d' % i)\n",
    "                    print(\"Train: \", train_loss_value)\n",
    "\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps, train_mode=False)\n",
    "                # Test\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                inps_ = sess.run(self.inps, feed_dict=feed_dict)\n",
    "\n",
    "                predictions = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "                \n",
    "                test_loss_value = 0.9 * test_loss_value + 0.1 * sess.run(test_loss, feed_dict=feed_dict)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Test: \", test_loss_value)\n",
    "\n",
    "                predictions_order = np.concatenate([np.expand_dims(prediction , 0) for prediction in predictions])\n",
    "                predictions_order = np.argmax(predictions_order, 2).transpose(1, 0)[:,0:FLAGS.max_steps]\n",
    "                    \n",
    "                input_order = np.concatenate([np.expand_dims(encoder_input_data_ , 0) for encoder_input_data_ in encoder_input_data])\n",
    "                input_order = np.argsort(input_order, 0).squeeze().transpose(1, 0)+1\n",
    "                \n",
    "                correct_order += np.sum(np.all(predictions_order == input_order,\n",
    "                                    axis=1))\n",
    "                all_order += FLAGS.batch_size\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print('Correct order / All order: %f' % (correct_order / all_order))\n",
    "                    correct_order = 0\n",
    "                    all_order = 0\n",
    "                    \n",
    "                    # print(encoder_input_data, decoder_input_data, targets_data)\n",
    "                    # print(inps_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-e56af08139da>:114 in step.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From <ipython-input-3-e56af08139da>:115 in step.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:From <ipython-input-3-e56af08139da>:116 in step.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Step: 0\n",
      "Train:  0.0\n",
      "Test:  2.64118080139\n",
      "Correct order / All order: 0.000000\n",
      "Step: 100\n",
      "Train:  0.0\n",
      "Test:  23.5542861279\n",
      "Correct order / All order: 0.000000\n",
      "Step: 200\n",
      "Train:  0.0\n",
      "Test:  20.7995675689\n",
      "Correct order / All order: 0.000000\n",
      "Step: 300\n",
      "Train:  0.0\n",
      "Test:  18.228292245\n",
      "Correct order / All order: 0.000000\n",
      "Step: 400\n",
      "Train:  0.0\n",
      "Test:  15.0995652318\n",
      "Correct order / All order: 0.000000\n",
      "Step: 500\n",
      "Train:  0.0\n",
      "Test:  13.75468547\n",
      "Correct order / All order: 0.000000\n",
      "Step: 600\n",
      "Train:  0.0\n",
      "Test:  12.2327765589\n",
      "Correct order / All order: 0.000937\n",
      "Step: 700\n",
      "Train:  0.0\n",
      "Test:  10.9781571831\n",
      "Correct order / All order: 0.003125\n",
      "Step: 800\n",
      "Train:  0.0\n",
      "Test:  10.1758886347\n",
      "Correct order / All order: 0.005938\n",
      "Step: 900\n",
      "Train:  0.0\n",
      "Test:  9.58192369965\n",
      "Correct order / All order: 0.008438\n",
      "Step: 1000\n",
      "Train:  0.0\n",
      "Test:  9.65656436603\n",
      "Correct order / All order: 0.007500\n",
      "Step: 1100\n",
      "Train:  0.0\n",
      "Test:  8.95585666672\n",
      "Correct order / All order: 0.010625\n",
      "Step: 1200\n",
      "Train:  0.0\n",
      "Test:  8.52878489405\n",
      "Correct order / All order: 0.014687\n",
      "Step: 1300\n",
      "Train:  0.0\n",
      "Test:  8.3247138444\n",
      "Correct order / All order: 0.018437\n",
      "Step: 1400\n",
      "Train:  0.0\n",
      "Test:  7.96206343818\n",
      "Correct order / All order: 0.030625\n",
      "Step: 1500\n",
      "Train:  0.0\n",
      "Test:  7.49278217901\n",
      "Correct order / All order: 0.033438\n",
      "Step: 1600\n",
      "Train:  0.0\n",
      "Test:  7.41911547123\n",
      "Correct order / All order: 0.040000\n",
      "Step: 1700\n",
      "Train:  0.0\n",
      "Test:  6.87055347784\n",
      "Correct order / All order: 0.046250\n",
      "Step: 1800\n",
      "Train:  0.0\n",
      "Test:  6.87427201631\n",
      "Correct order / All order: 0.046875\n",
      "Step: 1900\n",
      "Train:  0.0\n",
      "Test:  6.82593724921\n",
      "Correct order / All order: 0.055000\n",
      "Step: 2000\n",
      "Train:  0.0\n",
      "Test:  6.64779352641\n",
      "Correct order / All order: 0.071563\n",
      "Step: 2100\n",
      "Train:  0.0\n",
      "Test:  6.6005140044\n",
      "Correct order / All order: 0.078125\n",
      "Step: 2200\n",
      "Train:  0.0\n",
      "Test:  6.52944757527\n",
      "Correct order / All order: 0.073438\n",
      "Step: 2300\n",
      "Train:  0.0\n",
      "Test:  6.34285636096\n",
      "Correct order / All order: 0.084687\n",
      "Step: 2400\n",
      "Train:  0.0\n",
      "Test:  6.16837170759\n",
      "Correct order / All order: 0.092813\n",
      "Step: 2500\n",
      "Train:  0.0\n",
      "Test:  6.00057031496\n",
      "Correct order / All order: 0.085000\n",
      "Step: 2600\n",
      "Train:  0.0\n",
      "Test:  6.14060213203\n",
      "Correct order / All order: 0.091250\n",
      "Step: 2700\n",
      "Train:  0.0\n",
      "Test:  6.07893416003\n",
      "Correct order / All order: 0.104063\n",
      "Step: 2800\n",
      "Train:  0.0\n",
      "Test:  6.11823786159\n",
      "Correct order / All order: 0.090313\n",
      "Step: 2900\n",
      "Train:  0.0\n",
      "Test:  5.81205857821\n",
      "Correct order / All order: 0.105313\n",
      "Step: 3000\n",
      "Train:  0.0\n",
      "Test:  6.07865279908\n",
      "Correct order / All order: 0.096875\n",
      "Step: 3100\n",
      "Train:  0.0\n",
      "Test:  5.97248602601\n",
      "Correct order / All order: 0.094687\n",
      "Step: 3200\n",
      "Train:  0.0\n",
      "Test:  5.93453870668\n",
      "Correct order / All order: 0.106563\n",
      "Step: 3300\n",
      "Train:  0.0\n",
      "Test:  5.79188906786\n",
      "Correct order / All order: 0.105938\n",
      "Step: 3400\n",
      "Train:  0.0\n",
      "Test:  5.75669602213\n",
      "Correct order / All order: 0.100000\n",
      "Step: 3500\n",
      "Train:  0.0\n",
      "Test:  5.52833331197\n",
      "Correct order / All order: 0.111562\n",
      "Step: 3600\n",
      "Train:  0.0\n",
      "Test:  6.02682663392\n",
      "Correct order / All order: 0.101562\n",
      "Step: 3700\n",
      "Train:  0.0\n",
      "Test:  5.63087719352\n",
      "Correct order / All order: 0.114375\n",
      "Step: 3800\n",
      "Train:  0.0\n",
      "Test:  5.78812070607\n",
      "Correct order / All order: 0.110937\n",
      "Step: 3900\n",
      "Train:  0.0\n",
      "Test:  5.66657504341\n",
      "Correct order / All order: 0.100625\n",
      "Step: 4000\n",
      "Train:  0.0\n",
      "Test:  6.36633358071\n",
      "Correct order / All order: 0.097812\n",
      "Step: 4100\n",
      "Train:  0.0\n",
      "Test:  5.53463338636\n",
      "Correct order / All order: 0.105000\n",
      "Step: 4200\n",
      "Train:  0.0\n",
      "Test:  5.89018698049\n",
      "Correct order / All order: 0.104688\n",
      "Step: 4300\n",
      "Train:  0.0\n",
      "Test:  5.85142646736\n",
      "Correct order / All order: 0.099375\n",
      "Step: 4400\n",
      "Train:  0.0\n",
      "Test:  6.06311729058\n",
      "Correct order / All order: 0.106875\n",
      "Step: 4500\n",
      "Train:  0.0\n",
      "Test:  5.84944828586\n",
      "Correct order / All order: 0.101562\n",
      "Step: 4600\n",
      "Train:  0.0\n",
      "Test:  5.77613976224\n",
      "Correct order / All order: 0.105938\n",
      "Step: 4700\n",
      "Train:  0.0\n",
      "Test:  5.40804211498\n",
      "Correct order / All order: 0.102188\n",
      "Step: 4800\n",
      "Train:  0.0\n",
      "Test:  5.36277031372\n",
      "Correct order / All order: 0.105313\n",
      "Step: 4900\n",
      "Train:  0.0\n",
      "Test:  5.43172741506\n",
      "Correct order / All order: 0.108750\n",
      "Step: 5000\n",
      "Train:  0.0\n",
      "Test:  5.3043000637\n",
      "Correct order / All order: 0.113437\n",
      "Step: 5100\n",
      "Train:  0.0\n",
      "Test:  5.46390733729\n",
      "Correct order / All order: 0.114375\n",
      "Step: 5200\n",
      "Train:  0.0\n",
      "Test:  4.98989414655\n",
      "Correct order / All order: 0.120000\n",
      "Step: 5300\n",
      "Train:  0.0\n",
      "Test:  5.13553782451\n",
      "Correct order / All order: 0.125625\n",
      "Step: 5400\n",
      "Train:  0.0\n",
      "Test:  5.50030637943\n",
      "Correct order / All order: 0.121875\n",
      "Step: 5500\n",
      "Train:  0.0\n",
      "Test:  5.08224240104\n",
      "Correct order / All order: 0.121875\n",
      "Step: 5600\n",
      "Train:  0.0\n",
      "Test:  5.36293862231\n",
      "Correct order / All order: 0.120938\n",
      "Step: 5700\n",
      "Train:  0.0\n",
      "Test:  4.89472458023\n",
      "Correct order / All order: 0.111562\n",
      "Step: 5800\n",
      "Train:  0.0\n",
      "Test:  5.07557182448\n",
      "Correct order / All order: 0.112812\n",
      "Step: 5900\n",
      "Train:  0.0\n",
      "Test:  5.25892748371\n",
      "Correct order / All order: 0.106875\n",
      "Step: 6000\n",
      "Train:  0.0\n",
      "Test:  5.06987744296\n",
      "Correct order / All order: 0.117813\n",
      "Step: 6100\n",
      "Train:  0.0\n",
      "Test:  4.92131059725\n",
      "Correct order / All order: 0.118125\n",
      "Step: 6200\n",
      "Train:  0.0\n",
      "Test:  4.90002423821\n",
      "Correct order / All order: 0.116562\n",
      "Step: 6300\n",
      "Train:  0.0\n",
      "Test:  4.96162478224\n",
      "Correct order / All order: 0.111250\n",
      "Step: 6400\n",
      "Train:  0.0\n",
      "Test:  4.86207640593\n",
      "Correct order / All order: 0.130625\n",
      "Step: 6500\n",
      "Train:  0.0\n",
      "Test:  4.9242406792\n",
      "Correct order / All order: 0.120938\n",
      "Step: 6600\n",
      "Train:  0.0\n",
      "Test:  4.5409891207\n",
      "Correct order / All order: 0.139063\n",
      "Step: 6700\n",
      "Train:  0.0\n",
      "Test:  4.57241036216\n",
      "Correct order / All order: 0.129062\n",
      "Step: 6800\n",
      "Train:  0.0\n",
      "Test:  5.15589514213\n",
      "Correct order / All order: 0.115625\n",
      "Step: 6900\n",
      "Train:  0.0\n",
      "Test:  5.14282960012\n",
      "Correct order / All order: 0.115625\n",
      "Step: 7000\n",
      "Train:  0.0\n",
      "Test:  4.68238862772\n",
      "Correct order / All order: 0.125625\n",
      "Step: 7100\n",
      "Train:  0.0\n",
      "Test:  4.47681616886\n",
      "Correct order / All order: 0.127812\n",
      "Step: 7200\n",
      "Train:  0.0\n",
      "Test:  4.84903952204\n",
      "Correct order / All order: 0.132500\n",
      "Step: 7300\n",
      "Train:  0.0\n",
      "Test:  4.60777511953\n",
      "Correct order / All order: 0.133125\n",
      "Step: 7400\n",
      "Train:  0.0\n",
      "Test:  4.59431591091\n",
      "Correct order / All order: 0.145000\n",
      "Step: 7500\n",
      "Train:  0.0\n",
      "Test:  4.48117281412\n",
      "Correct order / All order: 0.150000\n",
      "Step: 7600\n",
      "Train:  0.0\n",
      "Test:  4.17711383564\n",
      "Correct order / All order: 0.152500\n",
      "Step: 7700\n",
      "Train:  0.0\n",
      "Test:  4.51148942334\n",
      "Correct order / All order: 0.152500\n",
      "Step: 7800\n",
      "Train:  0.0\n",
      "Test:  4.82260643535\n",
      "Correct order / All order: 0.141563\n",
      "Step: 7900\n",
      "Train:  0.0\n",
      "Test:  4.66987698377\n",
      "Correct order / All order: 0.127812\n",
      "Step: 8000\n",
      "Train:  0.0\n",
      "Test:  4.54704425979\n",
      "Correct order / All order: 0.149062\n",
      "Step: 8100\n",
      "Train:  0.0\n",
      "Test:  4.51470585182\n",
      "Correct order / All order: 0.131562\n",
      "Step: 8200\n",
      "Train:  0.0\n",
      "Test:  4.57919463983\n",
      "Correct order / All order: 0.150938\n",
      "Step: 8300\n",
      "Train:  0.0\n",
      "Test:  4.77423007591\n",
      "Correct order / All order: 0.133437\n",
      "Step: 8400\n",
      "Train:  0.0\n",
      "Test:  4.18372377237\n",
      "Correct order / All order: 0.142813\n",
      "Step: 8500\n",
      "Train:  0.0\n",
      "Test:  4.73296464061\n",
      "Correct order / All order: 0.139687\n",
      "Step: 8600\n",
      "Train:  0.0\n",
      "Test:  4.31552827121\n",
      "Correct order / All order: 0.149062\n",
      "Step: 8700\n",
      "Train:  0.0\n",
      "Test:  4.31770412166\n",
      "Correct order / All order: 0.150000\n",
      "Step: 8800\n",
      "Train:  0.0\n",
      "Test:  4.35069928553\n",
      "Correct order / All order: 0.147187\n",
      "Step: 8900\n",
      "Train:  0.0\n",
      "Test:  4.76979803333\n",
      "Correct order / All order: 0.136875\n",
      "Step: 9000\n",
      "Train:  0.0\n",
      "Test:  4.20033820976\n",
      "Correct order / All order: 0.146875\n",
      "Step: 9100\n",
      "Train:  0.0\n",
      "Test:  4.61365930275\n",
      "Correct order / All order: 0.148750\n",
      "Step: 9200\n",
      "Train:  0.0\n",
      "Test:  4.51976603396\n",
      "Correct order / All order: 0.144375\n",
      "Step: 9300\n",
      "Train:  0.0\n",
      "Test:  4.41383962695\n",
      "Correct order / All order: 0.156250\n",
      "Step: 9400\n",
      "Train:  0.0\n",
      "Test:  4.46562366589\n",
      "Correct order / All order: 0.139687\n",
      "Step: 9500\n",
      "Train:  0.0\n",
      "Test:  4.48749539292\n",
      "Correct order / All order: 0.144687\n",
      "Step: 9600\n",
      "Train:  0.0\n",
      "Test:  4.44788762234\n",
      "Correct order / All order: 0.137500\n",
      "Step: 9700\n",
      "Train:  0.0\n",
      "Test:  4.33171249466\n",
      "Correct order / All order: 0.148438\n",
      "Step: 9800\n",
      "Train:  0.0\n",
      "Test:  4.40068356053\n",
      "Correct order / All order: 0.135313\n",
      "Step: 9900\n",
      "Train:  0.0\n",
      "Test:  4.47880999151\n",
      "Correct order / All order: 0.152812\n"
     ]
    }
   ],
   "source": [
    "pointer_network = PointerNetwork(FLAGS.max_steps, 1, FLAGS.rnn_size, 1, 5, FLAGS.batch_size, 1e-2, 0.95)\n",
    "dataset = DataGenerator()\n",
    "pointer_network.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
